---
title: "Take Home Exercise 3"
subtitle: "Uncover illegal, unreported, and unregulated (IUU) fishing activities through visual analytics"
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"
author: "Fangxian"
execute: 
  eval: true
  warning: false
date: "4 June 2023"
date-modified: "`r Sys.Date()`"
---

# Task

Using the data provide by the VAST challenge, we are looking into the [Mini-Challenge 3](https://vast-challenge.github.io/2023/MC3.html) (MC3) to identify compaines possibly engaged in illegal, unreported, and unregulated (IUU) fishing.

# Data Wraggling

## Load Packages

```{r}
pacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, graphlayouts, ggforce,skimr,tidytext, tidyverse,igraph, topicmodels,tm)
```

## Data Import

In the code chunk below, `fromJSON()` of **jsonlite** package is used to import *MC3.json* into R environment.

```{r}
mc3_data <- fromJSON("data/MC3.json")
```

Examine the data, this is not a directed graph, not looking into in- and out-degree of the nodes.

## Extracting edges

Below code chunk changes the links field into character field.

```{r}
mc3_edges <- as_tibble(mc3_data$links)%>%
  distinct() %>%
  mutate(source = as.character(source),
         target = as.character(target),
         type = as.character(type)) %>%
  group_by(source, target, type) %>%
    summarise(weights = n()) %>%
  filter(source!=target)%>%
  ungroup
```

## Extracting nodes

```{r}
mc3_nodes <- as_tibble(mc3_data$nodes) %>%
#  distinct()%>%
  mutate(country = as.character(country),
         id = as.character(id),
         product_services = as.character(product_services),
         revenue_omu = as.numeric(as.character(revenue_omu)),
         type = as.character(type)) %>%
    select(id, country, type, revenue_omu, product_services)
```

# Initial Data Exploration

## Exploring the edges dataframe

In the code chunk below, [`skim()`](https://docs.ropensci.org/skimr/reference/skim.html) of [**skimr**](https://docs.ropensci.org/skimr/) package is used to display the summary statistics of *mc3_edges* tibble data frame.

```{r}
skim(mc3_edges)
```

The report above reveals that there is not missing values in all fields.

In the code chunk below, `datatable()` of DT package is used to display mc3_edges tibble data frame as an interactive table on the html document.

```{r}
DT::datatable(mc3_edges)
```

Below code chunks, counting number of companies a person owns and the number of owners a company has.

```{r}
ggplot(data = mc3_edges,
       aes(x=type)) +
  geom_bar()

unique_ids <- unique(mc3_edges$target)
num_unique_ids <- length(unique_ids)
num_unique_ids

Noofcompanies <- mc3_edges %>%
  group_by(target, source, type) %>%
  filter(type == "Beneficial Owner") %>%
  summarise(count=n()) %>%
  group_by(target)%>%
  summarise(count=sum(count))

psych::describe(Noofcompanies)
```

```{r}
Noofowners <- mc3_edges %>%
  group_by(source, target, type) %>%
  summarise(count=n()) %>%
  group_by(source)%>%
  summarise(count=sum(count))

psych::describe(Noofowners)
```

Below code chunk we are interested to see top 50 owners owning multiple companies, with John Smith and Michael Johnson have the highest of 9 companies to their name. This could be suspicious as why they need so many companies.

```{r}
list_top_50 <- Noofcompanies %>%
  arrange(desc(count)) %>%
  top_n(50, wt = count) 

ggplot(data = list_top_50, 
       aes(x = reorder(target, -count), y = count)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

## Exploring the nodes dataframe

```{r}
skim(mc3_nodes)
```

The report above reveals that there is no missing values in all fields.

In the code chunk below, `datatable()` of DT package is used to display mc3_nodes tibble data frame as an interactive table on the html document.

```{r}
DT::datatable(mc3_nodes)
```

Below code chunk to find out how is the distribution among the types of ownerhships.

```{r}
ggplot(data = mc3_nodes,
       aes(x = type)) +
  geom_bar()
```

Below code chunk we check on the revenue distribution among the types of ownerships.

```{r}
ggplot(data = mc3_nodes,
       aes(x= type,
         y = revenue_omu)) +
  geom_boxplot()
```

We combined the nodes and edges data so we can find out more on the owner-company relationships.

```{r}
combined <- left_join(mc3_nodes,mc3_edges,
                  by=c("id"="source"))
```

Below code chunk to find out more on which owners have high number of companies also generating a lot of revenue.

```{r}
combined <- combined %>%
  group_by(target, type.y, id, country, type.x, product_services)%>%
  summarize(revenue_omu) %>%
  filter(type.y == "Beneficial Owner")

filtered_combined <- combined %>%
  filter(target %in% list_top_50$target)%>%
  arrange(desc(revenue_omu))

ggplot(data = filtered_combined, 
       aes(x = target, y = revenue_omu)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

Michael Johnson, Mark Miller and James Rodriguez stand out from the above chart, below code we want to see what business they did that generate more revenue.

```{r}
Top_3_Revenue<- combined %>%
  filter (target %in% c("Michael Johnson", "Mark Miller","James Rodriguez")) %>%
  arrange(desc(revenue_omu))

DT::datatable(Top_3_Revenue)
```

## Insights

From the above data table, we see that Michael Johnson is involved in the fishing business and having many companies in different countries. The FishEye could probably look more into his business landscape across different companies and his business activities to understand more.

# Text Sensing with tidytext

## Simple word count

The code chunk below calculates number of times the word *fish* appeared in the field *product_services*.

```{r}
mc3_nodes %>% 
    mutate(n_fish = str_count(product_services, "fish")) 
```

## Tokenisation

The word tokenisation have different meaning in different scientific domains. In text sensing, **tokenisation** is the process of breaking up a given text into units called **tokens**. Tokens can be individual words, phrases or even whole sentences. In the process of tokenisation, some characters like punctuation marks may be discarded. The tokens usually become the input for the processes like parsing and text mining.

In the code chunk below, [`unnest_token()`](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html) of tidytext is used to split text in *product_services* field into words.

```{r}
token_nodes <- mc3_nodes %>%
  unnest_tokens(word, 
                product_services)
```

The two basic arguments to `unnest_tokens()` used here are column names. First we have the output column name that will be created as the text is unnested into it (*word*, in this case), and then the input column that the text comes from (*product_services*, in this case).

```{r}
token_nodes %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in product_services field")
```

The bar chart reveals that the unique words contains some words that may not be useful to use. For instance "a" and "to". In the word of text mining we call those words **stop words**. You want to remove these words from your analysis as they are fillers used to compose a sentence.

Using filter we also discover many "character(0)" which has no meaning in itself, we will also proceed to replace them with "NA".

## Removing stopwords

```{r}
token_nodes$word[token_nodes$word == "character"] <- "NA"
token_nodes$word[token_nodes$word == "0"] <- "NA"
```

```{r}
stopwords_removed <- token_nodes %>% 
  anti_join(stop_words)
```

```{r}
stopwords_removed %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in product_services field")
```

```{r}
stopwords_removed %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  filter(!word %in% head(word, 3)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in product_services field")

```

# Initial Network Visualization and Analysis

## Building network model with tidygraph

From the above text insights, we are interested to see the network of companies of *Beneficial Owners* with *fish* as their product services.

```{r}
mc3_nodes_fish <- stopwords_removed %>%
  filter(stopwords_removed$word == "fish")
```

```{r}
mc3_edges_fish <- mc3_edges[mc3_edges$source %in% mc3_nodes_fish$id,] %>%
  filter(type == "Beneficial Owner")

id1 <- mc3_edges_fish %>%
  select(source) %>%
  rename(id = source) 
id2 <- mc3_edges_fish %>% 
  select(target) %>% 
  rename(id = target) 
mc3_nodes_fish <- rbind(id1, id2) %>%
  distinct() %>% 
  left_join(mc3_nodes_fish,
            unmatched = "drop") 
```

```{r}
mc3_graph <- tbl_graph(nodes = mc3_nodes_fish,                        
                      edges = mc3_edges_fish,                        
                        directed = FALSE)  

mc3_graph<-mc3_graph%>%
  mutate(betweenness=centrality_betweenness())

mc3_graph
```

Using the distribution function to understand the centrality_betweenness().

```{r}
ggplot(as.data.frame(mc3_graph),aes(x=betweenness))+
  geom_histogram(bins=10,fill="lightblue",colour="black")+
  ggtitle("Distribution of centrality betweenness")+
  theme(plot.title = element_text(hjust=0.5))
```

Looking at this, we can filter our records where the centrality between is greater than 50 to understand the interactions.

```{r}
set.seed (1234)
degrees <- degree(mc3_graph)
V(mc3_graph)$degree <- degrees

mc3_graph %>%
  filter(betweenness >= 50) %>%
ggraph(layout = "fr") +
  geom_edge_link(aes(alpha=0.5)) +
  geom_node_point(aes(
    size = betweenness,
    colors = "lightblue",
    alpha = 0.5)) +
  scale_size_continuous(range=c(1,10))+
  geom_node_text(aes(label = id, filter= betweenness >=50 & degree >0), repel = TRUE)+
  theme_graph()
```

```{r}
list_top_30 <- Noofowners %>%
  arrange(desc(count)) %>%
  top_n(30, wt = count) 

ggplot(data = list_top_30, 
       aes(x = reorder(source, -count), y = count)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

# Topic Modelling

```{r}
corpus <- Corpus(VectorSource(stopwords_removed$word))

# Convert the corpus to a document-term matrix
dtm <- DocumentTermMatrix(corpus)

# Convert the document-term matrix to a tidy format
tidy_dtm <- tidy(dtm)
tidy_dtm

# vocabulary <- tidy_dtm %>%
#   filter(count > 1)
# 
# str(tidy_dtm)
# head(tidy_dtm)
# 
# # Create the LDA model
# lda_model <- LDA(tidy_dtm, k = 5, control = list(seed = 1234)) 

```

```{r}
# topics <- tidy(lda_model, matrix = "beta")
# 
# topics
```
