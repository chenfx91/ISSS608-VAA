---
title: "Take Home Exercise 3"
subtitle: "Uncover illegal, unreported, and unregulated (IUU) fishing activities through visual analytics"
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"
author: "Fangxian"
execute: 
  eval: true
  warning: false
date: "4 June 2023"
date-modified: "`r Sys.Date()`"
---

# Task

Using the data provide by the VAST challenge, we are looking into the [Mini-Challenge 3](https://vast-challenge.github.io/2023/MC3.html) (MC3) to identify compaines possibly engaged in illegal, unreported, and unregulated (IUU) fishing.

# Data Wraggling

## Load Packages

```{r}
pacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, graphlayouts, ggforce,skimr,tidytext, tidyverse,igraph, topicmodels,tm, stringr, ldatuning)
```

## Data Import

In the code chunk below, `fromJSON()` of **jsonlite** package is used to import *MC3.json* into R environment.

```{r}
mc3_data <- fromJSON("data/MC3.json")
```

Examine the data, this is not a directed graph, not looking into in- and out-degree of the nodes.

## Extracting edges

Below code chunk changes the links field into character field.

```{r}
mc3_edges <- as_tibble(mc3_data$links)%>%
  distinct() %>%
  mutate(source = as.character(source),
         target = as.character(target),
         type = as.character(type)) %>%
  group_by(source, target, type) %>%
    summarise(weights = n()) %>%
  filter(source!=target)%>%
  ungroup
```

## Extracting nodes

```{r}
mc3_nodes <- as_tibble(mc3_data$nodes) %>%
#  distinct()%>%
  mutate(country = as.character(country),
         id = as.character(id),
         product_services = as.character(product_services),
         revenue_omu = as.numeric(as.character(revenue_omu)),
         type = as.character(type)) %>%
    select(id, country, type, revenue_omu, product_services)
```

# Initial Data Exploration

## Exploring the edges dataframe

In the code chunk below, [`skim()`](https://docs.ropensci.org/skimr/reference/skim.html) of [**skimr**](https://docs.ropensci.org/skimr/) package is used to display the summary statistics of *mc3_edges* tibble data frame.

```{r}
skim(mc3_edges)
```

The report above reveals that there is not missing values in all fields.

In the code chunk below, `datatable()` of DT package is used to display mc3_edges tibble data frame as an interactive table on the html document.

```{r}
DT::datatable(mc3_edges)
```

Below code chunks, counting number of companies a person owns.

```{r}
ggplot(data = mc3_edges,
       aes(x=type)) +
  geom_bar()

unique_ids <- unique(mc3_edges$target)
num_unique_ids <- length(unique_ids)
num_unique_ids

Noofcompanies <- mc3_edges %>%
  group_by(target, source, type) %>%
  filter(type == "Beneficial Owner") %>%
  summarise(count=n()) %>%
  group_by(target)%>%
  summarise(count=sum(count))

psych::describe(Noofcompanies)
```

Below code chunk summarizes the numbers of owners to the company.

```{r}
Noofowners <- mc3_edges %>%
  group_by(source, target, type) %>%
  summarise(count=n()) %>%
  group_by(source)%>%
  summarise(count=sum(count))

psych::describe(Noofowners)
```

Below code chunk we are interested to see top 50 owners owning multiple companies, with John Smith and Michael Johnson have the highest of 9 companies to their name. This could be suspicious as why they need so many companies.

```{r}
list_top_50 <- Noofcompanies %>%
  arrange(desc(count)) %>%
  top_n(50, wt = count) 

ggplot(data = list_top_50, 
       aes(x = reorder(target, -count), y = count)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

## Exploring the nodes dataframe

```{r}
skim(mc3_nodes)
```

The report above reveals that there is no missing values in all fields.

In the code chunk below, `datatable()` of DT package is used to display mc3_nodes tibble data frame as an interactive table on the html document.

```{r}
DT::datatable(mc3_nodes)
```

Below code chunk to find out how is the distribution among the types of ownerhships.

```{r}
ggplot(data = mc3_nodes,
       aes(x = type)) +
  geom_bar()
```

Below code chunk we check on the revenue distribution among the types of ownerships.

```{r}
ggplot(data = mc3_nodes,
       aes(x= type,
         y = revenue_omu)) +
  geom_boxplot()
```

We combined the nodes and edges data so we can find out more on the owner-company relationships.

```{r}
combined <- left_join(mc3_nodes,mc3_edges,
                  by=c("id"="source"))
```

Below code chunk to find out more on which owners have high number of companies also generating a lot of revenue.

```{r}
combined <- combined %>%
  group_by(target, type.y, id, country, type.x, product_services)%>%
  summarize(revenue_omu) %>%
  filter(type.y == "Beneficial Owner")

filtered_combined <- combined %>%
  filter(target %in% list_top_50$target)%>%
  arrange(desc(revenue_omu))

ggplot(data = filtered_combined, 
       aes(x = target, y = revenue_omu)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

Michael Johnson, Mark Miller and James Rodriguez stand out from the above chart, below code we want to see what business they did that generate more revenue.

```{r}
Top_3_Revenue<- combined %>%
  filter (target %in% c("Michael Johnson", "Mark Miller","James Rodriguez")) %>%
  arrange(desc(revenue_omu))

DT::datatable(Top_3_Revenue)
```

## Insights

From the above data table, we see that Michael Johnson is involved in the fishing business and having many companies in different countries. The FishEye could probably look more into his business landscape across different companies and his business activities to understand more.

# Text Sensing with tidytext

## Simple word count

The code chunk below calculates number of times the word *fish* appeared in the field *product_services*.

```{r}
mc3_nodes %>% 
    mutate(n_fish = str_count(product_services, "fish")) 
```

## Tokenisation

The word tokenisation have different meaning in different scientific domains. In text sensing, **tokenisation** is the process of breaking up a given text into units called **tokens**. Tokens can be individual words, phrases or even whole sentences. In the process of tokenisation, some characters like punctuation marks may be discarded. The tokens usually become the input for the processes like parsing and text mining.

In the code chunk below, [`unnest_token()`](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html) of tidytext is used to split text in *product_services* field into words.

```{r}
token_nodes <- mc3_nodes %>%
  unnest_tokens(word, 
                product_services)
```

The two basic arguments to `unnest_tokens()` used here are column names. First we have the output column name that will be created as the text is unnested into it (*word*, in this case), and then the input column that the text comes from (*product_services*, in this case).

```{r}
token_nodes %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in product_services field")
```

The bar chart reveals that the unique words contains some words that may not be useful to use. For instance "a" and "to". In the word of text mining we call those words **stop words**. You want to remove these words from your analysis as they are fillers used to compose a sentence.

Using filter we also discover many "character(0)" which has no meaning in itself, we will also proceed to replace them with "NA".

## Removing stopwords

```{r}
token_nodes$word[token_nodes$word == "character"] <- "NA"
token_nodes$word[token_nodes$word == "0"] <- "NA"
```

```{r}
stopwords_removed <- token_nodes %>% 
  anti_join(stop_words)
```

```{r}
stopwords_removed %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in product_services field")
```

Below code removed the top 3 irrelevant words.

```{r}
stopwords_removed %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  filter(!word %in% head(word, 3)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in product_services field")

```

# Topic Modelling

This section, we will conduct topic modelling to find out the similar business groups. For this, we will make use of Latent Dirichlet Allocation (LDA).

First, run the below code chunk to remove the irrelevant top 3 keywords: "NA", "unknown", "products".

```{r}
stopwords_removed <- stopwords_removed %>%
  filter(!word %in% c("NA", "unknown", "products"))

dim(stopwords_removed)
```

## Converting Document-Term Matrix

We need to convert the document to a document-term matrix to to run LDA.

```{r}
# using as.matrix()
dtm <- stopwords_removed %>%
  count(id, word) %>%  # count each word used in each identified review 
  cast_dtm(id, word, n) %>%  # use the word counts by reviews  to create a DTM
  as.matrix()
```

```{r}
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```

## Finding optimal number of topics

```{r}
FindTopicsNumber_plot(result)
```

From the above graph, we see that after topic 12, for "Griffiths2004", there isn't much increase in the value, hence, we choose topics to be 12.

```{r}
# number of topics
K <- 12
# set random number generator seed
set.seed(1234)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
```

```{r}
lda_topics <- topicModel %>%
  tidy(matrix = "beta")
```

```{r}
lda_topics <- LDA(
  dtm,
  k = 12,
  method = "Gibbs",
  control = list(seed=42)
  ) %>%
  tidy(matrix = "beta")
word_probs <- lda_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  mutate(term2 = fct_reorder(term, beta))
ggplot(
  word_probs,
  aes(term2, beta, fill=as.factor(topic))
  ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

## Insights

From the above result, we can see topic 5 and topic 10 are most related to fishing industry. Topic 5 is a group of fishing business likely to have fresh sea products, while topic 10 is a group of fishing business likely to have more processed sea products. In the following network analysis session, we will focus on the nodes that contains topic 5 and topic 10 keywords.

# Initial Network Visualization and Analysis

## Building network model with tidygraph

From the above text insights, we are interested to see the network of companies of *Beneficial Owners* with topic 5 as their product services.

```{r}
mc3_nodes_topic5 <- stopwords_removed %>%
  filter(stopwords_removed$word %in% c("fish","salmon","tuna","shrimp","squid","frozen","sea","cod","fillets","crabs","fillet","pollock","lobster","smoked"))

mc3_nodes_topic10 <- stopwords_removed %>%
  filter(stopwords_removed$word %in% c("seafood","fish","fresh","processing","seafoods","marine","shellfish","frozen","aquatic","oils","canning","packing","fats","cured","preparation"))
```

```{r}
mc3_edges_topic5 <- mc3_edges[mc3_edges$source %in% mc3_nodes_topic5$id,] %>%   
  filter(type == "Beneficial Owner")  
id1 <- mc3_edges_topic5 %>%   
  select(source) %>%   
  rename(id = source) 
id2 <- mc3_edges_topic5 %>%    
  select(target) %>%    
  rename(id = target)  
mc3_nodes_topic5 <- rbind(id1, id2) %>%   
  distinct() %>%    
  left_join(mc3_nodes_topic5,             
            unmatched = "drop") 


mc3_edges_topic10 <- mc3_edges[mc3_edges$source %in% mc3_nodes_topic10$id,] %>%   
  filter(type == "Beneficial Owner")  
id1 <- mc3_edges_topic10 %>%   
  select(source) %>%   
  rename(id = source) 
id2 <- mc3_edges_topic10 %>%    
  select(target) %>%    
  rename(id = target)  
mc3_nodes_topic10 <- rbind(id1, id2) %>%   
  distinct() %>%    
  left_join(mc3_nodes_topic10,             
            unmatched = "drop") 
```

```{r}
mc3_graph_topic5 <- tbl_graph(nodes = mc3_nodes_topic5,                                               edges = mc3_edges_topic5,                                                 directed = FALSE)    
mc3_graph_topic5 <-mc3_graph_topic5 %>%   
  mutate(betweenness=centrality_betweenness())  

mc3_graph_topic10 <- tbl_graph(nodes = mc3_nodes_topic10,                                               edges = mc3_edges_topic10,                                                 directed = FALSE)    
mc3_graph_topic10 <-mc3_graph_topic10 %>%   
  mutate(betweenness=centrality_betweenness())  
```

Using the distribution function to understand the centrality_betweenness() for topic 5 and 10 graphs.

```{r}
ggplot(as.data.frame(mc3_graph_topic5),aes(x=betweenness))+
  geom_histogram(bins=10,fill="lightblue",colour="black")+
  ggtitle("Distribution of centrality betweenness for topic 5")+
  theme(plot.title = element_text(hjust=0.5))

ggplot(as.data.frame(mc3_graph_topic10),aes(x=betweenness))+
  geom_histogram(bins=10,fill="lightblue",colour="black")+
  ggtitle("Distribution of centrality betweenness for topic 10")+
  theme(plot.title = element_text(hjust=0.5))
```

Looking at this, we can filter our records where the centrality between is greater than 20 to understand the interactions.

```{r}
set.seed (1234)
degrees <- degree(mc3_graph_topic5)
V(mc3_graph_topic5)$degree <- degrees

mc3_graph_topic5 %>%
  filter(betweenness >= 75) %>%
ggraph(layout = "fr") +
  geom_edge_link(aes(alpha=0.5)) +
  geom_node_point(aes(
    size = betweenness,
    colors = "lightblue",
    alpha = 0.5)) +
  scale_size_continuous(range=c(1,10))+
  geom_node_text(aes(label = id, filter= betweenness >=200 &degree >0), repel = TRUE)+
  theme_graph()
```

```{r}
set.seed (1234)
degrees <- degree(mc3_graph_topic10)
V(mc3_graph_topic10)$degree <- degrees

mc3_graph_topic10 %>%
  filter(betweenness >= 75) %>%
ggraph(layout = "fr") +
  geom_edge_link(aes(alpha=0.5)) +
  geom_node_point(aes(
    size = betweenness,
    colors = "lightblue",
    alpha = 0.5)) +
  scale_size_continuous(range=c(1,10))+
  geom_node_text(aes(label = id, filter= betweenness >=200 &degree >0), repel = TRUE)+
  theme_graph()
```
